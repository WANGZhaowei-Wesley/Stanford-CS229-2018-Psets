\begin{answer}
At first, I think the description about well calibrated model is not accurate. If we choose an interval $(a, b)$ whose $\{i \in I_{a, b}\}$ only has positive examples(or negative samples). It's impossible for $\frac{\sum_{i\in I_{a, b}}P(y^{(i)}=1|x^{(i)};\theta)}{|\{i\in I_{a,b}\}|} = \frac{\sum_{i\in I_{a, b}}\mathbb{I}\{y^{(i)}=1\}}{|\{i\in I_{a, b}\}|}$ to hold true, because $P(y^{(i)}=1|x^{(i)};\theta)$ can't be 1(or 0). Thus, no model is perfectly calibrated.\\ \\
For a binary classification model, we should only consider intervals whose $\{i \in I_{a, b}\}$ has both positive and negative examples. If we consider a set $S$ of only one positive example and only one negative example, it's clear that a model that is perfectly calibrated doesn't imply that the model achieves perfect accuracy. For example, the model that outputs 0.8 for the negative example and 0.2 for the positive example is well calibrated, but it doesn't achieve perfect accuracy.\\ \\
Conversely, perfect accuracy doesn't lead to perfect calibration. We consider a train set of one positive example and one negative example again. If the model we got by training outputs 0.6 for the positive one and 0.1 for the negative one, the model achieves perfect accuracy. However, the property doesn't hold true.
\end{answer}
